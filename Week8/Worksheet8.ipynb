{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48210300-05fb-40d3-9bb1-b5a0cb0a3bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2501387\n",
    "# np03cs4a240053\n",
    "# Rachit Manandhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa44c0c0-753e-4e5a-9c66-5709ade9bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e798ffd-0386-4a41-928a-88b1a164d525",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 2. Custom Vs Scikit Learn Built Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb31ac-0763-4e26-a430-005e96cc9cbe",
   "metadata": {},
   "source": [
    "## Step 1 : Building a Custom Decision Tree with Information Gain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6aa1e9-b342-4fca-a879-44e163fbb10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Initializes the decision tree with the specified maximum depth.\n",
    "        Parameters:\n",
    "        max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until all\n",
    "        leaves are pure or contain fewer than the minimum samples required to split.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains the decision tree model using the provided training data.\n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix (n_samples, n_features) for training the model.\n",
    "        y (array-like): Target labels (n_samples,) for training the model.\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively builds the decision tree by splitting the data based on the best feature and threshold\n",
    "        .\n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix (n_samples, n_features) for splitting.\n",
    "        y (array-like): Target labels (n_samples,) for splitting.\n",
    "        depth (int, optional): Current depth of the tree during recursion.\n",
    "        Returns:\n",
    "        dict: A dictionary representing the structure of the tree, containing the best feature index,\n",
    "        threshold, and recursive tree nodes.\n",
    "        \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        \n",
    "        # Stopping conditions: pure node or reached max depth\n",
    "        if len(unique_classes) == 1:\n",
    "            return {'class': unique_classes[0]}\n",
    "        if num_samples == 0 or (self.max_depth and depth >= self.max_depth):\n",
    "            return {'class': np.bincount(y).argmax()}\n",
    "        \n",
    "        # Find the best split based on Information Gain (using Entropy)\n",
    "        best_info_gain = -float('inf')\n",
    "        best_split = None\n",
    "        for feature_idx in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature_idx] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                left_y = y[left_mask]\n",
    "                right_y = y[right_mask]\n",
    "                \n",
    "                info_gain = self._information_gain(y, left_y, right_y)\n",
    "                \n",
    "                if info_gain > best_info_gain:\n",
    "                    best_info_gain = info_gain\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': threshold,\n",
    "                        'left_y': left_y,\n",
    "                        'right_y': right_y,\n",
    "                    }\n",
    "        \n",
    "        if best_split is None:\n",
    "            return {'class': np.bincount(y).argmax()}\n",
    "        \n",
    "        # Recursively build the left and right subtrees\n",
    "        left_tree = self._build_tree(X[best_split['left_y']], best_split['left_y'], depth + 1)\n",
    "        right_tree = self._build_tree(X[best_split['right_y']], best_split['right_y'], depth + 1)\n",
    "        \n",
    "        return {'feature_idx': best_split['feature_idx'], 'threshold': best_split['threshold'], 'left_tree': left_tree, 'right_tree': right_tree}\n",
    "\n",
    "\n",
    "    def _information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Computes the Information Gain between the parent node and the left/right child nodes.\n",
    "        Parameters:\n",
    "        parent (array-like): The labels of the parent node.\n",
    "        left (array-like): The labels of the left child node.\n",
    "        right (array-like): The labels of the right child node.\n",
    "        Returns:\n",
    "        float: The Information Gain of the split.\n",
    "        \"\"\"\n",
    "        parent_entropy = self._entropy(parent)\n",
    "        left_entropy = self._entropy(left)\n",
    "        right_entropy = self._entropy(right)\n",
    "        \n",
    "        # Information Gain = Entropy(parent) - (weighted average of left and right entropies)\n",
    "        weighted_avg_entropy = (len(left) / len(parent)) * left_entropy + (len(right) / len(parent)) * right_entropy\n",
    "        return parent_entropy - weighted_avg_entropy\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes the entropy of a set of labels.\n",
    "        Parameters:\n",
    "        y (array-like): The labels for which entropy is calculated.\n",
    "        Returns:\n",
    "        float: The entropy of the labels.\n",
    "        \"\"\"\n",
    "        # Calculate the probability of each class\n",
    "        class_probs = np.bincount(y) / len(y)\n",
    "        \n",
    "        # Compute the entropy using the formula: -sum(p * log2(p))\n",
    "        return -np.sum(class_probs * np.log2(class_probs + 1e-9)) # Added small epsilon to avoid log(0)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the target labels for the given test data based on the trained decision tree.\n",
    "        Parameters:\n",
    "        X (array-like): Feature matrix (n_samples, n_features) for prediction.\n",
    "        Returns:\n",
    "        list: A list of predicted target labels (n_samples,).\n",
    "        \"\"\"\n",
    "        return [self._predict_single(x, self.tree) for x in X]\n",
    "\n",
    "\n",
    "    def _predict_single(self, x, tree):\n",
    "        \"\"\"\n",
    "        Recursively predicts the target label for a single sample by traversing the tree.\n",
    "        Parameters:\n",
    "        x (array-like): A single feature vector for prediction.\n",
    "        tree (dict): The current subtree or node to evaluate.\n",
    "        Returns:\n",
    "        int: The predicted class label for the sample.\n",
    "        \"\"\"\n",
    "        if 'class' in tree:\n",
    "            return tree['class']\n",
    "        \n",
    "        feature_val = x[tree['feature_idx']]\n",
    "        if feature_val <= tree['threshold']:\n",
    "            return self._predict_single(x, tree['left_tree'])\n",
    "        else:\n",
    "            return self._predict_single(x, tree['right_tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656da4a8-fcfc-48fc-91e1-ed0ca7dc1eea",
   "metadata": {},
   "source": [
    "## Step 2 : Load and Split the Iris Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02b19740-5e65-49d6-ab70-c9ff288146cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93071467-42f6-401d-9adb-c2843cc450c4",
   "metadata": {},
   "source": [
    "## Step 3 : Train and Evaluate a Custom Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a0c74b-66e6-4b7e-87e5-a2561383861c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Decision Tree Accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Train the custom decision tree\n",
    "custom_tree = CustomDecisionTree(max_depth=3)\n",
    "custom_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_custom = custom_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c8085-3c87-46f7-96bf-4c7520ae0c5c",
   "metadata": {},
   "source": [
    "## Step 4 : Train and Evaluate a Scikit Learn Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d89080ea-14d4-4eba-8d77-7945feff2591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn Decision Tree Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the Scikit-learn decision tree\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "sklearn_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaab9a3-e527-4481-b721-c78adcb3e680",
   "metadata": {},
   "source": [
    "## Step 5 : Result Comparision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e910cf3-067e-4c45-940c-2c1d000355dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Comparison:\n",
      "Custom Decision Tree: 0.8000\n",
      "Scikit-learn Decision Tree: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy Comparison:\")\n",
    "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
    "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc0ca8-5810-4c84-8fb1-1f7c4aada032",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# 3. Exercise - Ensemble Methods and Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b82866-aaab-4b0a-80ed-986c0cfec628",
   "metadata": {},
   "source": [
    "## 1. Implement Classification Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfcae9c-7d99-4dbc-8b3e-8f00f367aa0c",
   "metadata": {},
   "source": [
    "### Step 1: Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bcfef12-425a-4939-b511-768cd2bf564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_wine(return_X_y=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f7269-a691-4029-becb-2c21522308d3",
   "metadata": {},
   "source": [
    "### Step 2: Training Decision Tree Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72693a8d-2634-4565-aaf7-ca3c121cc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "f1_dt = f1_score(y_test, y_pred_dt, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5594c7-912f-4357-97cf-1400e48fedc8",
   "metadata": {},
   "source": [
    "### Step 3: Training Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f02d7de-df20-4a5d-9415-95e5d511d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "f1_rf = f1_score(y_test, y_pred_rf, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c379683f-2402-4723-b37a-7b3226620b63",
   "metadata": {},
   "source": [
    "### Step 4: Comparing F1 Scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cbd35e2-ab48-480e-ad42-2961b354f9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree F1 Score: 0.9449614374099499\n",
      "Random Forest F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree F1 Score:\", f1_dt)\n",
    "print(\"Random Forest F1 Score:\", f1_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5776095-4e73-4078-84d4-5203efdf2838",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f64fb3-b27f-4cd8-95bd-f40fe2e4ef12",
   "metadata": {},
   "source": [
    "### Step 1: Identifying the three hyperparameters of Random Tree Classifier:\n",
    "1. <b>n_estimators</b> - number of trees\n",
    "2. <b>max_depth</b> - maximum tree depth\n",
    "3. <b>min_sample_split</b> - minimum samples to split a node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b1c4f3-5fe6-4a4e-8201-a9d571efe536",
   "metadata": {},
   "source": [
    "### Step 2: Using GridSearchCV to optimize these hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e768c5b4-2818-4180-8350-cd5e0fe51b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Optimized F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_weighted',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "y_pred_best = best_rf_clf.predict(X_test)\n",
    "print(\"Optimized F1 Score:\", f1_score(y_test, y_pred_best, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4bd71-5152-4d2e-838a-e6b6c0e364b7",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 3. Implement Regression Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be93368-0812-4965-99dd-4c69896ea43b",
   "metadata": {},
   "source": [
    "### Step 1: Preparing Regression Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bdd8417-c67d-481e-903c-5b8585a58fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use alcohol feature as target\n",
    "X_reg = X[:, 1:]\n",
    "y_reg = X[:, 0]\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_reg, y_reg, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5fa2b-ed0c-49b8-a56b-7db6941a6ead",
   "metadata": {},
   "source": [
    "### Step 2: Training Decision Tree Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8d33318-1957-4881-a916-15ad42d28a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "pred_dt_reg = dt_reg.predict(Xr_test)\n",
    "mse_dt = mean_squared_error(yr_test, pred_dt_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d05599-4d0a-49ac-8bc7-b4eccb306066",
   "metadata": {},
   "source": [
    "### Step 3: Training Random Forest Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2676bb3-ea99-4949-a9d1-93eab9a466ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 0.31197222222222226\n",
      "Random Forest MSE: 0.15426672999999946\n"
     ]
    }
   ],
   "source": [
    "rf_reg = RandomForestRegressor(random_state=42)\n",
    "rf_reg.fit(Xr_train, yr_train)\n",
    "\n",
    "pred_rf_reg = rf_reg.predict(Xr_test)\n",
    "mse_rf = mean_squared_error(yr_test, pred_rf_reg)\n",
    "\n",
    "print(\"Decision Tree MSE:\", mse_dt)\n",
    "print(\"Random Forest MSE:\", mse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3463f63-0694-4c58-a07b-03401ecf67bf",
   "metadata": {},
   "source": [
    "### Step 4: Hyperparameter Tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c78f9675-d029-4c12-924b-05fe51be46a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 200, 'min_samples_leaf': 1, 'max_depth': 10}\n",
      "Optimized Random Forest MSE: 0.151070652463534\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(Xr_train, yr_train)\n",
    "\n",
    "best_rf_reg = random_search.best_estimator_\n",
    "\n",
    "pred_best_rf = best_rf_reg.predict(Xr_test)\n",
    "mse_best = mean_squared_error(yr_test, pred_best_rf)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Optimized Random Forest MSE:\", mse_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
